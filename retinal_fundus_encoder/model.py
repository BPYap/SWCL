import math

import torch
import torch.nn as nn
import torch.nn.functional as F

from retinal_fundus_encoder.deeplabv3plus import DeepLabv3Plus
from retinal_fundus_encoder.resnet import get_resnet_encoder, RESNET_OUT_CHANNELS
from retinal_fundus_encoder.unet import UNet

AVAILABLE_BACKBONES = ['resnet-18', 'resnet-34', 'resnet-50', 'resnet-101', 'resnet-152', 'resnetv2-50x1']
AVAILABLE_BACKBONES_SEGMENTATION = {
    'u-net': ['default', 'resnet-18', 'resnet-34', 'resnet-50', 'resnet-101', 'resnet-152'],
    'deeplabv3+': ['resnet-18', 'resnet-34', 'resnet-50', 'resnet-101', 'resnet-152', 'resnetv2-50x1']
}


class OutConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)

    def forward(self, x):
        return self.conv(x)


class StdLinear(nn.Linear):
    # Linear layer with weight standardization
    def forward(self, x):
        w = self.weight
        v, m = torch.var_mean(w, dim=1, keepdim=True, unbiased=False)
        w = (w - m) / torch.sqrt(v + 1e-10)

        return F.linear(x, w, self.bias)


class ProjectionNetwork(nn.Module):
    def __init__(self, in_channels, output_dim=128):
        super().__init__()

        self.out = nn.Sequential(
            nn.Linear(in_channels, in_channels),
            nn.ReLU(inplace=True),
            nn.Linear(in_channels, output_dim)
        )

    def forward(self, x):
        x = self.out(x)

        return F.normalize(x)


class BYOLProjectionNetwork(nn.Module):
    def __init__(self, in_channels, hidden_dim=4096, output_dim=256, use_ws_gn=True):
        super().__init__()
        linear = StdLinear if use_ws_gn else nn.Linear
        norm = nn.GroupNorm(16, hidden_dim) if use_ws_gn else nn.BatchNorm1d(hidden_dim)
        self.out = nn.Sequential(
            linear(in_channels, hidden_dim),
            norm,
            nn.ReLU(inplace=True),
            linear(hidden_dim, output_dim)
        )

    def forward(self, x):
        x = self.out(x)

        return x


class DINOProjectionNetwork(nn.Module):
    # adapted from https://github.com/facebookresearch/dino/blob/main/vision_transformer.py#L257
    def __init__(self, in_channels, nlayers=2, hidden_dim=2048, bottleneck_dim=256, output_dim=4096,
                 norm_last_layer=True):
        super().__init__()
        nlayers = max(nlayers, 1)
        if nlayers == 1:
            self.out = nn.Linear(in_channels, bottleneck_dim)
        else:
            layers = [nn.Linear(in_channels, hidden_dim), nn.ReLU()]
            for _ in range(nlayers - 2):
                layers.append(nn.Linear(hidden_dim, hidden_dim))
                layers.append(nn.ReLU())
            layers.append(nn.Linear(hidden_dim, bottleneck_dim))
            self.out = nn.Sequential(*layers)

        self.apply(self._init_weights)
        self.last_layer = nn.utils.weight_norm(nn.Linear(bottleneck_dim, output_dim, bias=False))
        self.last_layer.weight_g.data.fill_(1)
        if norm_last_layer:
            self.last_layer.weight_g.requires_grad = False

    @staticmethod
    def _no_grad_trunc_normal_(tensor, mean, std, a, b):
        # Cut & paste from PyTorch official master until it's in a few official releases - RW
        # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
        def norm_cdf(x):
            # Computes standard normal cumulative distribution function
            return (1. + math.erf(x / math.sqrt(2.))) / 2.

        with torch.no_grad():
            # Values are generated by using a truncated uniform distribution and
            # then using the inverse CDF for the normal distribution.
            # Get upper and lower cdf values
            l = norm_cdf((a - mean) / std)
            u = norm_cdf((b - mean) / std)

            # Uniformly fill tensor with values from [l, u], then translate to
            # [2l-1, 2u-1].
            tensor.uniform_(2 * l - 1, 2 * u - 1)

            # Use inverse cdf transform for normal distribution to get truncated
            # standard normal
            tensor.erfinv_()

            # Transform to proper mean, std
            tensor.mul_(std * math.sqrt(2.))
            tensor.add_(mean)

            # Clamp to ensure it's in the proper range
            tensor.clamp_(min=a, max=b)
            return tensor

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            self._no_grad_trunc_normal_(m.weight, mean=0., std=.02, a=-2., b=2.)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.out(x)
        x = nn.functional.normalize(x, dim=-1, p=2)
        x = self.last_layer(x)

        return x


class BaseEncoder(nn.Module):
    def __init__(self, n_channels):
        """
        Args:
            n_channels (int): Number of channels in input images.
        """
        super().__init__()

        if n_channels != 3:
            raise ValueError("`n_channels` other than 3 is currently not supported.")

        self.n_channels = n_channels
        self.encoder = None

    def init(self, backbone, imagenet_init):
        """
        Args:
            backbone (str): Name of the encoder backbone.
            imagenet_init (bool): If available, choose whether to initialize the encoder with weights
                                  pre-trained on ImageNet.
        """
        self.encoder = get_resnet_encoder(backbone, imagenet_init, use_dilation=False)

    def forward(self, x):
        if self.encoder is None:
            raise RuntimeError("Model is not initialized. Call `init` to initialize the model.")

        return self.encoder(x)['out']


class JointClassificationModel(BaseEncoder):
    def __init__(self, n_channels, num_class_per_task):
        """
        Args:
            n_channels (int): Number of channels in input images.
            num_class_per_task (dict): Mapping of task name to the number of classes.
        """
        super().__init__(n_channels)

        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.num_class_per_task = num_class_per_task

        self.dropout_rate = 0.0
        self.out = nn.ModuleDict()

    def init(self, backbone, imagenet_init):
        super().init(backbone, imagenet_init)

        for task_name, num_classes in self.num_class_per_task.items():
            out = nn.Sequential(
                nn.Linear(RESNET_OUT_CHANNELS[backbone], num_classes)
            )
            self.out[task_name] = out

    def set_dropout_rate(self, dropout_rate=0.0):
        self.dropout_rate = dropout_rate

    def forward(self, x):
        feat_maps = super().forward(x)
        x = self.avg_pool(feat_maps)
        x = torch.flatten(x, 1)
        # x = F.dropout(x, p=self.dropout_rate, training=self.training)

        logits = dict()
        for task_name, out in self.out.items():
            logits[task_name] = out(F.dropout(x, p=self.dropout_rate, training=self.training))

        return feat_maps, logits


class ClassificationModel(JointClassificationModel):
    def __init__(self, n_channels, num_classes):
        """
        Args:
            n_channels (int): Number of channels in input images.
            num_classes (int): Number of classes.
        """
        super().__init__(n_channels, {'classification': num_classes})

    def forward(self, x):
        feat_maps, logits = super().forward(x)

        return feat_maps, logits['classification']


class ClassificationModelForCAM(ClassificationModel):
    def __init__(self, n_channels, num_classes):
        """
        Args:
            n_channels (int): Number of channels in input images.
            num_classes (int): Number of classes.
        """
        super().__init__(n_channels, num_classes)

    def init(self, backbone, imagenet_init):
        super().init(backbone, imagenet_init)

        encoder = self.encoder
        # reduce stride of last downsampling layer
        if hasattr(encoder, "layer4"):  # ResNetV1
            encoder.layer4[0].conv1.stride = 1
            encoder.layer4[0].downsample[0].stride = 1
        elif hasattr(encoder, "body"):  # ResNetV2
            encoder.body.block4.unit01.conv2.stride = 1
            encoder.body.block4.unit01.downsample.stride = 1


class ContrastiveLearningModel(BaseEncoder):
    def __init__(self, n_channels):
        """
        Args:
            n_channels (int): Number of channels in input images.
        """
        super().__init__(n_channels)

        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.projection_network = None

    def init(self, backbone, imagenet_init):
        super().init(backbone, imagenet_init)

        self.projection_network = ProjectionNetwork(RESNET_OUT_CHANNELS[backbone], 128)

    def forward(self, x):
        feat_maps = super().forward(x)

        return self.projection_network(self.avg_pool(feat_maps).flatten(1))


class BYOLModel(BaseEncoder):
    def __init__(self, n_channels, attach_predictor, output_dim=256):
        """
        Args:
            n_channels (int): Number of channels in input images.
        """
        super().__init__(n_channels)
        self.attach_predictor = attach_predictor
        self.output_dim = output_dim

        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.projection_network = None
        self.predictor_network = None

    def init(self, backbone, imagenet_init):
        super().init(backbone, imagenet_init)

        self.projection_network = BYOLProjectionNetwork(RESNET_OUT_CHANNELS[backbone], output_dim=self.output_dim)
        if self.attach_predictor:
            self.predictor_network = BYOLProjectionNetwork(in_channels=self.output_dim, output_dim=self.output_dim)

    def forward(self, x):
        feat_maps = super().forward(x)
        projections = self.projection_network(self.avg_pool(feat_maps).flatten(1))

        return self.predictor_network(projections) if self.predictor_network else projections


class DINOModel(BaseEncoder):
    def __init__(self, n_channels):
        """
        Args:
            n_channels (int): Number of channels in input images.
        """
        super().__init__(n_channels)

        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.projection_network = None

    def init(self, backbone, imagenet_init):
        super().init(backbone, imagenet_init)

        self.projection_network = DINOProjectionNetwork(RESNET_OUT_CHANNELS[backbone])

    def forward(self, x):
        feat_maps = super().forward(x)

        return self.projection_network(self.avg_pool(feat_maps).flatten(1))


def _get_out_channels(architecture, encoder_backbone):
    encoder_out_channels, decoder_out_channels = None, None
    if architecture == 'u-net':
        if encoder_backbone == 'default':
            encoder_out_channels = 1024
            decoder_out_channels = 64
        elif encoder_backbone in ['resnet-18', 'resnet-34']:
            encoder_out_channels = RESNET_OUT_CHANNELS[encoder_backbone]
            decoder_out_channels = 16
        elif encoder_backbone in ['resnet-50', 'resnet-101', 'resnet-152']:
            encoder_out_channels = RESNET_OUT_CHANNELS[encoder_backbone]
            decoder_out_channels = 64
    elif architecture == 'deeplabv3+':
        encoder_out_channels = RESNET_OUT_CHANNELS[encoder_backbone]
        decoder_out_channels = 256

    if not encoder_out_channels or not decoder_out_channels:
        raise ValueError(f"Unsupported architecture and backbone '{architecture}', '{encoder_backbone}'.")

    return encoder_out_channels, decoder_out_channels


class BaseEncoderDecoder(nn.Module):
    def __init__(self, n_channels):
        """
        Args:
            n_channels (int): Number of channels in input images.
        """
        super().__init__()
        self.n_channels = n_channels

        self.architecture = None
        self.encoder = None
        self.decoder = None
        self.encoder_out_channels = None
        self.decoder_out_channels = None

    def init(self, architecture, encoder_backbone, imagenet_init):
        """
        Args:
            architecture (str): Name of the encoder-decoder architecture.
            encoder_backbone (str): Name of the encoder backbone.
            imagenet_init (bool): If available, choose whether to initialize the encoder with weights
                                  pre-trained on ImageNet.
        """
        n_channels = self.n_channels

        if architecture == 'u-net':
            encoder_decoder = UNet(
                n_channels,
                bilinear=True,
                encoder_backbone=encoder_backbone,
                imagenet_init=imagenet_init
            )
        elif architecture == 'deeplabv3+':
            encoder_decoder = DeepLabv3Plus(
                n_channels,
                encoder_backbone=encoder_backbone,
                imagenet_init=imagenet_init
            )
        else:
            raise ValueError(f"Unknown architecture '{architecture}'")

        self.architecture = architecture
        self.encoder = encoder_decoder.encoder
        self.decoder = encoder_decoder.decoder
        self.encoder_out_channels, self.decoder_out_channels = _get_out_channels(architecture, encoder_backbone)

    def forward_encoder(self, x):
        architecture = self.architecture
        encoder = self.encoder

        if encoder is None:
            raise RuntimeError("Model is not initialized. Call `init` to initialize the model.")

        if architecture == 'u-net':
            # input x is the processed images tensor
            # output xs is a list of feature maps from each Down block
            xs = [x]
            for key, down in encoder.items():
                xs.append(down(xs[-1]))

            return xs[1:]
        elif architecture == 'deeplabv3+':
            # input x is the processed images tensor
            # output xs is a list of feature maps from the first and last backbone layer
            xs = encoder(x)
            xs = [xs['low_level'], xs['out']]

            return xs

    def forward_decoder(self, xs):
        architecture = self.architecture
        decoder = self.decoder

        if decoder is None:
            raise RuntimeError("Model is not initialized. Call `init` to initialize the model.")

        if architecture == 'u-net':
            # input xs is a list of feature maps from each Down block
            # output x is the decoded feature maps
            x = xs.pop()
            for up in decoder.values():
                x = up(x, xs.pop())

            return x
        elif architecture == 'deeplabv3+':
            # input xs is a list of feature maps from the first and last backbone layer
            # output x is the decoded feature maps
            x = decoder(xs)

            return x


class JointSegmentationModel(BaseEncoderDecoder):
    def __init__(self, n_channels, num_class_per_task):
        """
        Args:
            n_channels (int): Number of channels in input images.
            num_class_per_task (dict): Mapping of task name to the number of classes.
        """
        super().__init__(n_channels)

        self.num_class_per_task = num_class_per_task
        self.out = nn.ModuleDict()

    def init(self, architecture, encoder_backbone, imagenet_init):
        super().init(architecture, encoder_backbone, imagenet_init)

        for task_name, num_classes in self.num_class_per_task.items():
            out = OutConv(self.decoder_out_channels, num_classes)
            self.out[task_name] = out

    def forward(self, x):
        # segment images using encoder and decoder
        input_shape = x.shape[-2:]
        feature_maps = self.forward_decoder(self.forward_encoder(x))
        logits = dict()
        for task_name, out in self.out.items():
            temp = out(feature_maps)
            if temp.shape[-2:] != input_shape:
                temp = F.interpolate(temp, size=input_shape, mode='bilinear', align_corners=False)
            logits[task_name] = temp

        return logits


class SegmentationModel(JointSegmentationModel):
    def __init__(self, n_channels, num_classes):
        """
        Args:
            n_channels (int): Number of channels in input images.
            num_classes (int): Number of classes.
        """
        super().__init__(n_channels, {'segmentation': num_classes})

    def forward(self, x):
        return super().forward(x)['segmentation']
